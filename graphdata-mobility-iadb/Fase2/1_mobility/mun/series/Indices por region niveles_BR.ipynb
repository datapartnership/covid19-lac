{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparar ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/enum.py\n"
     ]
    }
   ],
   "source": [
    "import enum\n",
    "print(enum.__file__)  \n",
    "# standard library location should be something like \n",
    "# /usr/local/lib/python3.6/enum.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping enum34 as it is not installed.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip uninstall -y enum34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: awswrangler in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (2.8.0)\n",
      "Requirement already satisfied: botocore<2.0.0,>=1.15.49 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from awswrangler) (1.20.87)\n",
      "Requirement already satisfied: pymysql<1.1.0,>=0.9.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from awswrangler) (1.0.2)\n",
      "Requirement already satisfied: pyarrow<4.1.0,>=2.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from awswrangler) (4.0.0)\n",
      "Requirement already satisfied: pg8000<1.20.0,>=1.16.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from awswrangler) (1.19.5)\n",
      "Requirement already satisfied: redshift-connector~=2.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from awswrangler) (2.0.881)\n",
      "Requirement already satisfied: openpyxl~=3.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from awswrangler) (3.0.6)\n",
      "Requirement already satisfied: numpy<1.21.0,>=1.18.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from awswrangler) (1.19.5)\n",
      "Requirement already satisfied: pandas<1.3.0,>=1.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from awswrangler) (1.1.5)\n",
      "Requirement already satisfied: boto3<2.0.0,>=1.12.49 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from awswrangler) (1.17.87)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3<2.0.0,>=1.12.49->awswrangler) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.5.0,>=0.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3<2.0.0,>=1.12.49->awswrangler) (0.4.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore<2.0.0,>=1.15.49->awswrangler) (2.8.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore<2.0.0,>=1.15.49->awswrangler) (1.26.5)\n",
      "Requirement already satisfied: et-xmlfile in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from openpyxl~=3.0.0->awswrangler) (1.0.1)\n",
      "Requirement already satisfied: jdcal in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from openpyxl~=3.0.0->awswrangler) (1.4.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pandas<1.3.0,>=1.1.0->awswrangler) (2021.1)\n",
      "Requirement already satisfied: scramp==1.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pg8000<1.20.0,>=1.16.0->awswrangler) (1.4.0)\n",
      "Requirement already satisfied: asn1crypto==1.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from scramp==1.4.0->pg8000<1.20.0,>=1.16.0->awswrangler) (1.4.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<2.0.0,>=1.15.49->awswrangler) (1.15.0)\n",
      "Requirement already satisfied: lxml>=4.6.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from redshift-connector~=2.0.0->awswrangler) (4.6.3)\n",
      "Requirement already satisfied: requests<2.25.2,>=2.23.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from redshift-connector~=2.0.0->awswrangler) (2.25.1)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.7.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from redshift-connector~=2.0.0->awswrangler) (4.9.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from beautifulsoup4<5.0.0,>=4.7.0->redshift-connector~=2.0.0->awswrangler) (2.0.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<2.25.2,>=2.23.0->redshift-connector~=2.0.0->awswrangler) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<2.25.2,>=2.23.0->redshift-connector~=2.0.0->awswrangler) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<2.25.2,>=2.23.0->redshift-connector~=2.0.0->awswrangler) (2020.12.5)\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install awswrangler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyAthena in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (2.3.0)\n",
      "Requirement already satisfied: tenacity>=4.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from PyAthena) (7.0.0)\n",
      "Requirement already satisfied: botocore>=1.5.52 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from PyAthena) (1.20.87)\n",
      "Requirement already satisfied: boto3>=1.4.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from PyAthena) (1.17.87)\n",
      "Requirement already satisfied: s3transfer<0.5.0,>=0.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3>=1.4.4->PyAthena) (0.4.2)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3>=1.4.4->PyAthena) (0.10.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore>=1.5.52->PyAthena) (1.26.5)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore>=1.5.52->PyAthena) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from python-dateutil<3.0.0,>=2.1->botocore>=1.5.52->PyAthena) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install PyAthena"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conectar con pyathena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyathena.pandas.cursor import PandasCursor\n",
    "from pyathena import connect\n",
    "import pandas as pd\n",
    "import time\n",
    "# agregar directorio en el cual se almacenan las tablas que se van a consultar\n",
    "directorio = 's3://iadbprod-csd-hub-analyticaldata/graphdata-mobility-temporal/athena-results/'\n",
    "# base de datos\n",
    "bd = 'graphdata' # Database in Glue\n",
    "cursor = connect(s3_staging_dir = directorio, region_name = 'us-east-1', schema_name = bd, cursor_class = PandasCursor).cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import awswrangler as wr\n",
    "import numpy as np\n",
    "import calendar\n",
    "import boto3\n",
    "s3 = boto3.resource('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "import shutil\n",
    "from io import StringIO\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeclaraciÃ³n funciones para Impo-Expo s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dataframe_to_csv_on_s3(dataframe, bucket, filename):\n",
    "    \"\"\" Write a dataframe to a CSV on S3 \"\"\"\n",
    "    # Create buffer\n",
    "    csv_buffer = StringIO()\n",
    "    # Write dataframe to buffer\n",
    "    dataframe.to_csv(csv_buffer, sep=\";\" , line_terminator='\\n')\n",
    "    # Create S3 object\n",
    "    s3_resource = boto3.resource(\"s3\")\n",
    "    # Write buffer to S3 object\n",
    "    s3_resource.Object(bucket, filename).put(Body=csv_buffer.getvalue())\n",
    "    print(\"Writing {} records to {}\".format(len(dataframe), filename))\n",
    "    print(\"S3\")\n",
    "    print(dataframe.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dataframe_to_excel_on_s3(dataframe, bucket, filename):\n",
    "    \"\"\" Write a dataframe to a excel on S3 \"\"\"\n",
    "    with io.BytesIO() as output:\n",
    "        with pd.ExcelWriter(output, engine='xlsxwriter') as writer:\n",
    "            dataframe.to_excel(writer,index=False)\n",
    "        data = output.getvalue()\n",
    "\n",
    "    s3 = boto3.resource('s3')\n",
    "    s3.Bucket(bucket).put_object(Key=filename, Body=data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_dataframe_csv_from_s3(bucket, filename):\n",
    "    client = boto3.client('s3')\n",
    "    bucket_name = bucket\n",
    "    object_key = filename\n",
    "    csv_obj = client.get_object(Bucket=bucket_name, Key=object_key)\n",
    "    body = csv_obj['Body']\n",
    "    csv_string = body.read().decode('utf-8')\n",
    "    df = pd.read_csv(StringIO(csv_string),sep=\";\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_dataframe_excel_from_s3(bucket, filename):\n",
    "    client = boto3.client('s3')\n",
    "    bucket_name = bucket\n",
    "    object_key = filename\n",
    "    obj = client.get_object(Bucket=bucket_name, Key=object_key)\n",
    "    data = obj['Body'].read()\n",
    "    df = pd.read_excel(io.BytesIO(data), encoding='utf-8')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_file_to_s3(bucket,file_local,file_s3):\n",
    "    s3.Bucket(bucket).upload_file(file_local,file_s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importacion de Delta de Fase 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fecha_inicio = '02-01-2020'\n",
    "fecha_final = '04-30-2020'\n",
    "fechas = pd.date_range(fecha_inicio, fecha_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pais = 'BR'\n",
    "nivel_1 = 'id_provincia'\n",
    "nivel_4 = 'cd_geocmi'\n",
    "filtro_variable = \"n_obs\"\n",
    "filtro_num = \"10\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computar serie de usuarios homogenea\n",
    "filtro_variable2 = 'n_obs_madrugada'\n",
    "filtro_variable22 = 'n_obs_noche'\n",
    "filtro_num2 = 4\n",
    "#consulta athena\n",
    "tabla_usuarios_distancia =  f'historico_usuarios_regiones_{pais}'   \n",
    "query_filtro = (f''' SELECT caid,count(*) as count_filtro\n",
    "                     FROM {tabla_usuarios_distancia} \n",
    "                     WHERE (({tabla_usuarios_distancia}.{filtro_variable2} + {tabla_usuarios_distancia}.{filtro_variable22})>= {filtro_num2})\n",
    "                     GROUP BY caid\n",
    "                     HAVING count(*)>=30''')\n",
    "df_filtro = cursor.execute(query_filtro).as_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21, 72 seg\n",
      "22, 67 seg\n",
      "23, 72 seg\n",
      "24, 72 seg\n",
      "25, 74 seg\n",
      "26, 75 seg\n",
      "27, 73 seg\n",
      "28, 72 seg\n",
      "29, 77 seg\n",
      "210, 73 seg\n",
      "211, 70 seg\n",
      "212, 74 seg\n",
      "213, 73 seg\n",
      "214, 70 seg\n",
      "215, 71 seg\n",
      "216, 75 seg\n",
      "217, 79 seg\n",
      "218, 74 seg\n",
      "219, 73 seg\n",
      "220, 71 seg\n",
      "221, 69 seg\n",
      "222, 68 seg\n",
      "223, 56 seg\n",
      "224, 69 seg\n",
      "225, 72 seg\n",
      "226, 67 seg\n",
      "227, 67 seg\n",
      "228, 72 seg\n",
      "229, 67 seg\n",
      "31, 67 seg\n",
      "32, 70 seg\n",
      "33, 78 seg\n",
      "34, 84 seg\n",
      "35, 72 seg\n",
      "36, 73 seg\n",
      "37, 76 seg\n",
      "38, 84 seg\n",
      "39, 68 seg\n",
      "310, 69 seg\n",
      "311, 66 seg\n",
      "312, 68 seg\n",
      "313, 69 seg\n",
      "314, 70 seg\n",
      "315, 70 seg\n",
      "316, 72 seg\n",
      "317, 66 seg\n",
      "318, 63 seg\n",
      "319, 69 seg\n",
      "320, 65 seg\n",
      "321, 67 seg\n",
      "322, 61 seg\n",
      "323, 64 seg\n",
      "324, 70 seg\n",
      "325, 48 seg\n",
      "326, 49 seg\n",
      "327, 45 seg\n",
      "328, 44 seg\n",
      "329, 44 seg\n",
      "330, 46 seg\n",
      "331, 46 seg\n",
      "41, 50 seg\n",
      "42, 50 seg\n",
      "43, 45 seg\n",
      "44, 45 seg\n",
      "45, 43 seg\n",
      "46, 50 seg\n",
      "47, 46 seg\n",
      "48, 46 seg\n",
      "49, 46 seg\n",
      "410, 44 seg\n",
      "411, 43 seg\n",
      "412, 43 seg\n",
      "413, 43 seg\n",
      "414, 45 seg\n",
      "415, 45 seg\n",
      "416, 49 seg\n",
      "417, 48 seg\n",
      "418, 47 seg\n",
      "419, 45 seg\n",
      "420, 48 seg\n",
      "421, 50 seg\n",
      "422, 50 seg\n",
      "423, 56 seg\n",
      "424, 55 seg\n",
      "425, 55 seg\n",
      "426, 52 seg\n",
      "427, 56 seg\n",
      "428, 59 seg\n",
      "429, 53 seg\n",
      "430, 54 seg\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "for fecha in fechas:\n",
    "    time_fecha = time.time()\n",
    "    dia = str(fecha.day)\n",
    "    mes = str(fecha.month)\n",
    "    query_fecha = \"\"\n",
    "    day = str(fecha.day)\n",
    "    if len(day) == 1:\n",
    "        day = f'0{day}'\n",
    "    codigo_fecha = str(fecha.month) + day\n",
    "    tabla_usuarios_distancia =  f'historico_usuarios_regiones_{pais}'   \n",
    "    query_filtro = (f''' SELECT *\n",
    "                         FROM {tabla_usuarios_distancia} \n",
    "                         WHERE ({tabla_usuarios_distancia}.month={mes}\n",
    "                         AND {tabla_usuarios_distancia}.day={dia} \n",
    "                         AND {tabla_usuarios_distancia}.iso_country_code ='{pais}' \n",
    "                         AND {tabla_usuarios_distancia}.{filtro_variable} >= {filtro_num})''')\n",
    "    df = cursor.execute(query_filtro).as_pandas()\n",
    "    df['MasDe1km'] = df['distancia_recorrida'].apply(lambda x: 1 if x>=1 else 0)\n",
    "    \n",
    "    \n",
    "    ##Filtro mas tenue\n",
    "    #nivel 1\n",
    "    df_by_day_1=df.groupby(['month','day',nivel_1]).agg({'MasDe1km':'sum','caid':'count'}).reset_index()\n",
    "    df_by_day_1=df_by_day_1.rename(columns={'caid':'n_users','month':'Mes','day':'Dia'})\n",
    "    df_by_day_1['%MasDe1km'] = df_by_day_1['MasDe1km']/df_by_day_1['n_users']*100  \n",
    "    df_median=df[df['MasDe1km']==1].groupby(['month','day',nivel_1]).agg({'distancia_recorrida':'median'}).reset_index()\n",
    "    df_median=df_median.rename(columns={'distancia_recorrida':'Mediana1KM','month':'Mes','day':'Dia'})\n",
    "    df_by_day_1=df_by_day_1.merge(df_median, how='left',on=['Mes','Dia',nivel_1])\n",
    "    df_by_day_1[nivel_1]=df_by_day_1[nivel_1].astype('int')\n",
    "    df_by_day_1['Mes']=df_by_day_1['Mes'].astype('int')\n",
    "    df_by_day_1['Dia']=df_by_day_1['Dia'].astype('int')\n",
    "    df_by_day_1[\"Dia\"]=df_by_day_1[\"Dia\"].map(\"{:02}\".format)\n",
    "    df_by_day_1[\"key4\"]=df_by_day_1['Mes'].astype(str)+ df_by_day_1[\"Dia\"].astype(str)\n",
    "    df_by_day_1['key4']=df_by_day_1['key4'].astype('int')\n",
    "    df_by_day_1['Dia']=df_by_day_1['Dia'].astype('int')    \n",
    "    if (nivel_1==\"id_provincia\"):\n",
    "        df_by_day_1=df_by_day_1.rename(columns={nivel_1:'adm1'})\n",
    "    #if (dia=='1' and mes=='2'):\n",
    "    #    df_final_1=df_by_day_1\n",
    "    #else:\n",
    "    #    df_final_1=df_final_1.append(df_by_day_1)\n",
    "    \n",
    "    \n",
    "    table_uc = f'historico_latlong_{pais}_{codigo_fecha}_nivel1'\n",
    "    wr.s3.to_parquet(  # Storing the data and metadata to Data Lake\n",
    "    df = df_by_day_1,\n",
    "    dataset = True,\n",
    "    table = table_uc,\n",
    "    database = \"graphdata\",\n",
    "    path = f\"s3://iadbprod-csd-hub-analyticaldata/graphdata-mobility-temporal/Results_niveles/Temp_BR/{table_uc}\"\n",
    "    )\n",
    "    \n",
    "    \n",
    "    #nivel 4   \n",
    "    df_by_day_4=df.groupby(['month','day',nivel_4]).agg({'MasDe1km':'sum','caid':'count'}).reset_index()\n",
    "    df_by_day_4=df_by_day_4.rename(columns={'caid':'n_users','month':'Mes','day':'Dia'})\n",
    "    df_by_day_4['%MasDe1km'] = df_by_day_4['MasDe1km']/df_by_day_4['n_users']*100  \n",
    "    df_median=df[df['MasDe1km']==1].groupby(['month','day',nivel_4]).agg({'distancia_recorrida':'median'}).reset_index()\n",
    "    df_median=df_median.rename(columns={'distancia_recorrida':'Mediana1KM','month':'Mes','day':'Dia'})\n",
    "    df_by_day_4=df_by_day_4.merge(df_median, how='left',on=['Mes','Dia',nivel_4])    \n",
    "    df_by_day_4[nivel_4]=df_by_day_4[nivel_4].astype('int')\n",
    "    df_by_day_4['Mes']=df_by_day_4['Mes'].astype('int')\n",
    "    df_by_day_4['Dia']=df_by_day_4['Dia'].astype('int')\n",
    "    df_by_day_4[\"Dia\"]=df_by_day_4[\"Dia\"].map(\"{:02}\".format)\n",
    "    df_by_day_4[\"key4\"]=df_by_day_4['Mes'].astype(str)+ df_by_day_4[\"Dia\"].astype(str)\n",
    "    df_by_day_4['key4']=df_by_day_4['key4'].astype('int')\n",
    "    df_by_day_4['Dia']=df_by_day_4['Dia'].astype('int')\n",
    "    if (nivel_4==\"cd_geocmi\"):\n",
    "        df_by_day_4=df_by_day_4.rename(columns={nivel_4:'adm4'})\n",
    "    #if (dia=='1' and mes=='2'):\n",
    "    #    df_final_4=df_by_day_4\n",
    "    #else:\n",
    "    #    df_final_4=df_final_4.append(df_by_day_4)\n",
    "        \n",
    "    table_uc = f'historico_latlong_{pais}_{codigo_fecha}_nivel4'\n",
    "    wr.s3.to_parquet(  # Storing the data and metadata to Data Lake\n",
    "    df = df_by_day_4,\n",
    "    dataset = True,\n",
    "    table = table_uc,\n",
    "    database = \"graphdata\",\n",
    "    path = f\"s3://iadbprod-csd-hub-analyticaldata/graphdata-mobility-temporal/Results_niveles/Temp_BR/{table_uc}\"\n",
    "    )\n",
    "    \n",
    "    \n",
    "    ##Filtro severo\n",
    "    #nivel 1\n",
    "    df2=df.merge(df_filtro,how='inner',on='caid')    \n",
    "    df_by_day_11=df2.groupby(['month','day',nivel_1]).agg({'MasDe1km':'sum','caid':'count'}).reset_index()\n",
    "    df_by_day_11=df_by_day_11.rename(columns={'caid':'n_users','month':'Mes','day':'Dia'})\n",
    "    df_by_day_11['%MasDe1km'] = df_by_day_11['MasDe1km']/df_by_day_11['n_users']*100  \n",
    "    df_median=df2[df2['MasDe1km']==1].groupby(['month','day',nivel_1]).agg({'distancia_recorrida':'median'}).reset_index()\n",
    "    df_median=df_median.rename(columns={'distancia_recorrida':'Mediana1KM','month':'Mes','day':'Dia'})\n",
    "    df_by_day_11=df_by_day_11.merge(df_median, how='left',on=['Mes','Dia',nivel_1])      \n",
    "    df_by_day_11[nivel_1]=df_by_day_11[nivel_1].astype('int')\n",
    "    df_by_day_11['Mes']=df_by_day_11['Mes'].astype('int')\n",
    "    df_by_day_11['Dia']=df_by_day_11['Dia'].astype('int')\n",
    "    df_by_day_11[\"Dia\"]=df_by_day_11[\"Dia\"].map(\"{:02}\".format)\n",
    "    df_by_day_11[\"key4\"]=df_by_day_11['Mes'].astype(str)+ df_by_day_11[\"Dia\"].astype(str)\n",
    "    df_by_day_11['key4']=df_by_day_11['key4'].astype('int')\n",
    "    df_by_day_11['Dia']=df_by_day_11['Dia'].astype('int')    \n",
    "    if (nivel_1==\"id_provincia\"):\n",
    "        df_by_day_11=df_by_day_11.rename(columns={nivel_1:'adm1'})\n",
    "    #if (dia=='1' and mes=='2'):\n",
    "    #    df_final_fs1=df_by_day_11\n",
    "    #else:\n",
    "    #    df_final_fs1=df_final_fs1.append(df_by_day_11)\n",
    "    \n",
    "    table_uc = f'historico_latlong_{pais}_{codigo_fecha}_nivel1_fs'\n",
    "    wr.s3.to_parquet(  # Storing the data and metadata to Data Lake\n",
    "    df = df_by_day_11,\n",
    "    dataset = True,\n",
    "    table = table_uc,\n",
    "    database = \"graphdata\",\n",
    "    path = f\"s3://iadbprod-csd-hub-analyticaldata/graphdata-mobility-temporal/Results_niveles/Temp_BR/{table_uc}\"\n",
    "    ) \n",
    "    \n",
    "    \n",
    "    #nivel 4\n",
    "    df2=df.merge(df_filtro,how='inner',on='caid')\n",
    "    df_by_day_44=df2.groupby(['month','day',nivel_4]).agg({'MasDe1km':'sum','caid':'count'}).reset_index()\n",
    "    df_by_day_44=df_by_day_44.rename(columns={'caid':'n_users','month':'Mes','day':'Dia'})\n",
    "    df_by_day_44['%MasDe1km'] = df_by_day_44['MasDe1km']/df_by_day_44['n_users']*100  \n",
    "    df_median=df2[df2['MasDe1km']==1].groupby(['month','day',nivel_4]).agg({'distancia_recorrida':'median'}).reset_index()\n",
    "    df_median=df_median.rename(columns={'distancia_recorrida':'Mediana1KM','month':'Mes','day':'Dia'})\n",
    "    df_by_day_44=df_by_day_44.merge(df_median, how='left',on=['Mes','Dia',nivel_4])      \n",
    "    df_by_day_44[nivel_4]=df_by_day_44[nivel_4].astype('int')\n",
    "    df_by_day_44['Mes']=df_by_day_44['Mes'].astype('int')\n",
    "    df_by_day_44['Dia']=df_by_day_44['Dia'].astype('int')\n",
    "    df_by_day_44[\"Dia\"]=df_by_day_44[\"Dia\"].map(\"{:02}\".format)\n",
    "    df_by_day_44[\"key4\"]=df_by_day_44['Mes'].astype(str)+ df_by_day_44[\"Dia\"].astype(str)\n",
    "    df_by_day_44['key4']=df_by_day_44['key4'].astype('int')\n",
    "    df_by_day_44['Dia']=df_by_day_44['Dia'].astype('int')\n",
    "    if (nivel_4==\"cd_geocmi\"):\n",
    "        df_by_day_44=df_by_day_44.rename(columns={nivel_4:'adm4'})\n",
    "    #if (dia=='1' and mes=='2'):\n",
    "    #    df_final_fs4=df_by_day_44\n",
    "    #else:\n",
    "    #    df_final_fs4=df_final_fs4.append(df_by_day_44)\n",
    "        \n",
    "    table_uc = f'historico_latlong_{pais}_{codigo_fecha}_nivel4_fs'\n",
    "    wr.s3.to_parquet(  # Storing the data and metadata to Data Lake\n",
    "    df = df_by_day_44,\n",
    "    dataset = True,\n",
    "    table = table_uc,\n",
    "    database = \"graphdata\",\n",
    "    path = f\"s3://iadbprod-csd-hub-analyticaldata/graphdata-mobility-temporal/Results_niveles/Temp_BR/{table_uc}\"\n",
    "    )  \n",
    "    \n",
    "\n",
    "    tiempo = (time.time()-time_fecha)\n",
    "    print(f\"{mes}{dia}, {int(tiempo)} seg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Control - Filtro 10p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cod_region=pd.read_excel('cod_BR.xlsx', engine='openpyxl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creo adm1 de referencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5088905970255535\n"
     ]
    }
   ],
   "source": [
    "df_final = []\n",
    "start_time = time.time()\n",
    "for i,fecha in enumerate(fechas):\n",
    "    time_fecha = time.time()\n",
    "    day = str(fecha.day)\n",
    "    if len(day) == 1:\n",
    "        day = f'0{day}'\n",
    "    codigo_fecha = str(fecha.month) + day\n",
    "    #print(codigo_fecha)\n",
    "    tabla_usuarios_fecha = f'historico_latlong_{pais}_{codigo_fecha}_nivel1'\n",
    "    query = f'''SELECT * FROM {tabla_usuarios_fecha}'''\n",
    "    #print(f\"{codigo_fecha}\")\n",
    "    df2 = cursor.execute(query).as_pandas()\n",
    "    if (codigo_fecha=='201'):\n",
    "        df_final=df2\n",
    "    else:\n",
    "        df_final=df_final.append(df2)\n",
    "\n",
    "df_final_1 = df_final\n",
    "tiempo = (time.time()-time_fecha)/60\n",
    "print((time.time()-start_time)/60)\n",
    "df_final_1=df_final_1.rename(columns={'mas_de1km':'MasDe1km','_mas_de1km':'%MasDe1km','mediana1_km':'Mediana1KM','mes':'Mes','dia':'Dia'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Mes', 'Dia', 'adm1', 'MasDe1km', 'n_users', '%MasDe1km', 'Mediana1KM',\n",
       "       'key4'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final_1.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### control nivel adm4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.301035459836324\n"
     ]
    }
   ],
   "source": [
    "df_final = []\n",
    "start_time = time.time()\n",
    "for i,fecha in enumerate(fechas):\n",
    "    time_fecha = time.time()\n",
    "    day = str(fecha.day)\n",
    "    if len(day) == 1:\n",
    "        day = f'0{day}'\n",
    "    codigo_fecha = str(fecha.month) + day\n",
    "    #print(codigo_fecha)\n",
    "    tabla_usuarios_fecha = f'historico_latlong_{pais}_{codigo_fecha}_nivel4'\n",
    "    query = f'''SELECT * FROM {tabla_usuarios_fecha}'''\n",
    "    #print(f\"{codigo_fecha}\")\n",
    "    df2 = cursor.execute(query).as_pandas()\n",
    "    if (codigo_fecha=='201'):\n",
    "        df_final=df2\n",
    "    else:\n",
    "        df_final=df_final.append(df2)\n",
    "df_final=df_final.rename(columns={'mas_de1km':'MasDe1km','_mas_de1km':'%MasDe1km','mediana1_km':'Mediana1KM','mes':'Mes','dia':'Dia'})\n",
    "df_final_4 = df_final\n",
    "tiempo = (time.time()-time_fecha)/60\n",
    "print((time.time()-start_time)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9998978465736418 CONTINUE\n"
     ]
    }
   ],
   "source": [
    "## control nivel adm4\n",
    "base_control=df_final_1\n",
    "base_control_v4=df_final_4\n",
    "base_control_v4[\"adm1\"]=\"\"\n",
    "base_control_v4.loc[base_control_v4[\"adm4\"].astype(str).str.len()==7,[\"adm1\"]]=base_control_v4[\"adm4\"][base_control_v4[\"adm4\"].astype(str).str.len()==7].map(lambda x: str(x)[0:2])\n",
    "base_control4=base_control_v4.groupby(['Mes','Dia','adm1']).agg({'MasDe1km':'sum','n_users':'sum','%MasDe1km':'mean','Mediana1KM':'median'}).reset_index()\n",
    "base_control4['adm1']=base_control4['adm1'].astype('int')\n",
    "base_control4['adm1_v1']=\"\"\n",
    "\n",
    "for ids in list(cod_region['id_ibge'].astype('int')):\n",
    "    base_control4.loc[base_control4['adm1']== ids , ['adm1_v1']] =  int(cod_region.loc[cod_region['id_ibge']== ids, 'id_provincia'])\n",
    "\n",
    "base_control4 =base_control4.loc[:,['Mes','Dia','adm1_v1','MasDe1km','n_users','%MasDe1km','Mediana1KM']]\n",
    "base_control4 =base_control4.rename(columns={'adm1_v1':'adm1','MasDe1km':'MasDe1km_adm4','n_users':'n_users_adm4','%MasDe1km':'%MasDe1km_adm4','Mediana1KM':'Mediana1KM_adm4'})\n",
    "\n",
    "base_contro4=base_control.merge(base_control4,how='left',on=['Mes','Dia','adm1'])\n",
    "base_contro4['%MasDe1km_adm4']=base_contro4['MasDe1km_adm4']/base_contro4['n_users_adm4']*100\n",
    "\n",
    "cora=base_contro4.groupby('adm1')[['MasDe1km','MasDe1km_adm4']].corr().unstack().iloc[:,1].min()\n",
    "corb=base_contro4.groupby('adm1')[['n_users','n_users_adm4']].corr().unstack().iloc[:,1].min()\n",
    "corc=base_contro4.groupby('adm1')[['%MasDe1km','%MasDe1km_adm4']].corr().unstack().iloc[:,1].min()\n",
    "\n",
    "if (min(cora,corb,corc)<0.9): print(\"OJO\") \n",
    "else: print(min(cora,corb,corc), \"CONTINUE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Guardar archivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 500585 records to graphdata-mobility-public/Results_niveles/BR/almenos10p/BR_amd4_10p.csv\n",
      "S3\n",
      "(500585, 9)\n"
     ]
    }
   ],
   "source": [
    "nivel_4 = 'amd4'\n",
    "ruta=f'graphdata-mobility-public/Results_niveles/{pais}/almenos10p/{pais}_{nivel_4}_10p.csv'\n",
    "write_dataframe_to_csv_on_s3(df_final_4, 'iadbprod-csd-hub-analyticaldata', ruta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtro Base Usuarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6818121989568073\n"
     ]
    }
   ],
   "source": [
    "df_final = []\n",
    "start_time = time.time()\n",
    "for i,fecha in enumerate(fechas):\n",
    "    time_fecha = time.time()\n",
    "    day = str(fecha.day)\n",
    "    if len(day) == 1:\n",
    "        day = f'0{day}'\n",
    "    codigo_fecha = str(fecha.month) + day\n",
    "    #print(codigo_fecha)\n",
    "    tabla_usuarios_fecha = f'historico_latlong_{pais}_{codigo_fecha}_nivel1_fs'\n",
    "    query = f'''SELECT * FROM {tabla_usuarios_fecha}'''\n",
    "    #print(f\"{codigo_fecha}\")\n",
    "    df2 = cursor.execute(query).as_pandas()\n",
    "    if (codigo_fecha=='201'):\n",
    "        df_final=df2\n",
    "    else:\n",
    "        df_final=df_final.append(df2)\n",
    "df_final=df_final.rename(columns={'mas_de1km':'MasDe1km','_mas_de1km':'%MasDe1km','mediana1_km':'Mediana1KM','mes':'Mes','dia':'Dia'})\n",
    "df_final_fs1 = df_final\n",
    "tiempo = (time.time()-time_fecha)/60\n",
    "print((time.time()-start_time)/60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Control adm4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "3.0294425964355467\n"
     ]
    }
   ],
   "source": [
    "df_final = []\n",
    "start_time = time.time()\n",
    "for i,fecha in enumerate(fechas):\n",
    "    time_fecha = time.time()\n",
    "    day = str(fecha.day)\n",
    "    if len(day) == 1:\n",
    "        day = f'0{day}'\n",
    "    codigo_fecha = str(fecha.month) + day\n",
    "    #print(codigo_fecha)\n",
    "    tabla_usuarios_fecha = f'historico_latlong_{pais}_{codigo_fecha}_nivel4_fs'\n",
    "    query = f'''SELECT * FROM {tabla_usuarios_fecha}'''\n",
    "    print(f\"{codigo_fecha}\")\n",
    "    df2 = cursor.execute(query).as_pandas()\n",
    "    if (codigo_fecha=='201'):\n",
    "        df_final=df2\n",
    "    else:\n",
    "        df_final=df_final.append(df2)\n",
    "df_final=df_final.rename(columns={'mas_de1km':'MasDe1km','_mas_de1km':'%MasDe1km','mediana1_km':'Mediana1KM','mes':'Mes','dia':'Dia'})\n",
    "df_final_fs4 = df_final\n",
    "tiempo = (time.time()-time_fecha)/60\n",
    "print((time.time()-start_time)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.999931858704129 CONTINUE\n"
     ]
    }
   ],
   "source": [
    "## control nivel adm4\n",
    "base_control=df_final_fs1\n",
    "base_control_v4=df_final_fs4\n",
    "base_control_v4[\"adm1\"]=\"\"\n",
    "base_control_v4.loc[base_control_v4[\"adm4\"].astype(str).str.len()==7,[\"adm1\"]]=base_control_v4[\"adm4\"][base_control_v4[\"adm4\"].astype(str).str.len()==7].map(lambda x: str(x)[0:2])\n",
    "base_control4=base_control_v4.groupby(['Mes','Dia','adm1']).agg({'MasDe1km':'sum','n_users':'sum','%MasDe1km':'mean','Mediana1KM':'median'}).reset_index()\n",
    "base_control4['adm1']=base_control4['adm1'].astype('int')\n",
    "base_control4['adm1_v1']=\"\"\n",
    "\n",
    "for ids in list(cod_region['id_ibge'].astype('int')):\n",
    "    base_control4.loc[base_control4['adm1']== ids , ['adm1_v1']] =  int(cod_region.loc[cod_region['id_ibge']== ids, 'id_provincia'])\n",
    "\n",
    "base_control4 =base_control4.loc[:,['Mes','Dia','adm1_v1','MasDe1km','n_users','%MasDe1km','Mediana1KM']]\n",
    "base_control4 =base_control4.rename(columns={'adm1_v1':'adm1','MasDe1km':'MasDe1km_adm4','n_users':'n_users_adm4','%MasDe1km':'%MasDe1km_adm4','Mediana1KM':'Mediana1KM_adm4'})\n",
    "\n",
    "base_contro4=base_control.merge(base_control4,how='left',on=['Mes','Dia','adm1'])\n",
    "base_contro4['%MasDe1km_adm4']=base_contro4['MasDe1km_adm4']/base_contro4['n_users_adm4']*100\n",
    "\n",
    "cora=base_contro4.groupby('adm1')[['MasDe1km','MasDe1km_adm4']].corr().unstack().iloc[:,1].min()\n",
    "corb=base_contro4.groupby('adm1')[['n_users','n_users_adm4']].corr().unstack().iloc[:,1].min()\n",
    "corc=base_contro4.groupby('adm1')[['%MasDe1km','%MasDe1km_adm4']].corr().unstack().iloc[:,1].min()\n",
    "\n",
    "if (min(cora,corb,corc)<0.9): print(\"OJO\") \n",
    "else: print(min(cora,corb,corc), \"CONTINUE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Guardar archivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 497937 records to graphdata-mobility-public/Results_niveles/BR/base_usuario/BR_cd_geocmi_baseuser.csv\n",
      "S3\n",
      "(497937, 9)\n"
     ]
    }
   ],
   "source": [
    "nivel_4 = 'adm4'\n",
    "ruta=f'graphdata-mobility-public/Results_niveles/{pais}/base_usuario/{pais}_{nivel_4}_baseuser.csv'\n",
    "write_dataframe_to_csv_on_s3(df_final_fs4, 'iadbprod-csd-hub-analyticaldata', ruta)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
